{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7987e-5bb6-4155-a93c-9901185e680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inside the 'quotation marks' the url address of the ToposText page\n",
    "url_topostext = ' ........ '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaa005-8a29-4235-bf4b-45680e256376",
   "metadata": {},
   "source": [
    "Navigate https://github.com/PerseusDL/canonical-greekLit/tree/master, open View Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b1a4a-650b-45c9-89d1-1b807a8e781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inside the 'quotation marks' the url address of the Perseus Github page\n",
    "url_perseus = ' ......... '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f6d85-f444-4613-9319-381a2cca8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## assign the name of the work\n",
    "work = ' ....... '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944f11d-6093-4fa2-bca9-d3cc1658438f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9631085-01e7-4165-a793-095287cbaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebe3ef-d181-4f86-a132-1ec14a941a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef125a-5b8e-4055-bb08-3485cf36ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a41fe-b9d7-40f9-94f7-688c231219ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = work\n",
    "subfolders = [\"books\", \"labse_embeddings\"]\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(main_folder, subfolder), exist_ok=True)\n",
    "os.makedirs('data/intermediate/alignments', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcd227-0532-4981-955a-4699a057d6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb81407-f6d4-4c66-bdf6-db8b05ce265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbe773-b3a2-4f8b-9f3e-b8ba5edc0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Classifier.load('ner-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ce7a4-06a5-4498-be3e-3cbf656b1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_topostext = requests.get(url_topostext)\n",
    "soup_topostext = BeautifulSoup(access_topostext.content, 'html.parser')\n",
    "topostext_page = soup_topostext.find_all('html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ba749-63b2-498f-a137-1ba810b00da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference_column = []\n",
    "book_column = []\n",
    "token_column = []\n",
    "flair_ner_column = []\n",
    "topostext_id_column = []\n",
    "\n",
    "for html in topostext_page:\n",
    "    divisions = html.find_all(\"div\", class_=\"pSelector\") \n",
    "    for div in divisions:   \n",
    "        p_tag_id = div.get(\"id\")\n",
    "        print(p_tag_id)\n",
    "        if p_tag_id != 'E':\n",
    "            match = re.match(r\"^\\d+\", p_tag_id)\n",
    "            if match:\n",
    "                book = int(match.group())\n",
    "            paragraph = div.find('p') \n",
    "            text = paragraph.get_text()\n",
    "            sentence = Sentence(text) \n",
    "            tagger.predict(sentence)\n",
    "            \n",
    "            reference_temp_column = []\n",
    "            book_temp_column = []\n",
    "            index_temp_column = []\n",
    "            token_temp_column = []\n",
    "            start_pos_temp_column = []\n",
    "            flair_ner_temp_column = []\n",
    "            topostext_id_temp_column = []\n",
    "            \n",
    "            topostext_id_special_temp_column = []\n",
    "            start_pos_special_temp_column = []\n",
    "            \n",
    "            for i,token in enumerate(sentence):\n",
    "                token_text = token.text\n",
    "                start_pos = token.start_position\n",
    "                \n",
    "                reference_temp_column.append(p_tag_id)\n",
    "                book_temp_column.append(book)\n",
    "                index_temp_column.append(i)\n",
    "                token_temp_column.append(token_text)\n",
    "                start_pos_temp_column.append(start_pos)\n",
    "                flair_ner_temp_column.append('O')\n",
    "                topostext_id_temp_column.append('-')\n",
    "            \n",
    "            for entity in sentence.get_spans('ner'):\n",
    "                entity_label = entity.labels[0].value\n",
    "                for i, tok in enumerate(entity):    \n",
    "                    start_pos_tok = tok.start_position\n",
    "                    if i == 0:\n",
    "                        tok_label = 'B-'+str(entity_label)\n",
    "                    else :\n",
    "                        tok_label = 'I-'+str(entity_label)\n",
    "                    for x,y in enumerate(start_pos_temp_column):\n",
    "                        if int(y) == int(start_pos_tok):\n",
    "                            flair_ner_temp_column[x] = tok_label\n",
    "                            \n",
    "            tags = paragraph.find_all('a')\n",
    "            for tag in tags:\n",
    "                if tag.get('about'):\n",
    "                    topostext_id = tag.get('about') \n",
    "                elif tag.get('href'):\n",
    "                    topostext_id = tag.get('href') \n",
    "                topostext_id_special_temp_column.append(topostext_id)  \n",
    "            \n",
    "            clean_text=str(paragraph) \n",
    "            clean_text = re.sub('<a[^>]*>', 'µ', clean_text) \n",
    "            clean_text = re.sub('</a>', '', clean_text) \n",
    "            clean_text = re.sub('<p[^>]*>', '', clean_text) \n",
    "            clean_text = re.sub('</p>', '', clean_text)\n",
    "            clean_text = re.sub('<br[^>]*>', '', clean_text) \n",
    "            clean_text = re.sub('</br>', '', clean_text)\n",
    "            clean_text = re.sub('<b[^>]*>', '', clean_text) \n",
    "            clean_text = re.sub('</b>', '', clean_text)\n",
    "            \n",
    "            list_special_char_seen = 0   \n",
    "            for i, character in enumerate(clean_text): \n",
    "                if character == \"µ\" :\n",
    "                    start_pos = int(i-list_special_char_seen)\n",
    "                    list_special_char_seen += 1 \n",
    "                    start_pos_special_temp_column.append(start_pos) \n",
    "            for i,start_pos_special in enumerate(start_pos_special_temp_column):\n",
    "                for n,start_pos in enumerate(start_pos_temp_column):\n",
    "                    if start_pos_special == start_pos:\n",
    "                        topostext_id_temp_column[n] = topostext_id_special_temp_column[i]\n",
    "            \n",
    "            reference_column.extend(reference_temp_column)\n",
    "            book_column.extend(book_temp_column)\n",
    "            token_column.extend(token_temp_column)\n",
    "            flair_ner_column.extend(flair_ner_temp_column)\n",
    "            topostext_id_column.extend(topostext_id_temp_column)\n",
    "            \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81565117-378a-411e-8978-1410bad1307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'reference':reference_column, 'book':book_column, 'token':token_column, 'flair_ner':flair_ner_column,'topostext':topostext_id_column}\n",
    "flairToposText = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20bdb2-45af-432f-9fe6-a5bafd37024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flairToposText['entity_id'] = '-'\n",
    "count_ent = -1\n",
    "for i,ent in enumerate(flairToposText['flair_ner']):\n",
    "    if len(ent) > 1:\n",
    "        if ent.startswith('B'):\n",
    "            count_ent = count_ent+1\n",
    "            flairToposText.loc[i,'entity_id'] = count_ent \n",
    "        else : flairToposText.loc[i, 'entity_id'] = count_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518c26e-2525-49ab-ad7f-3b5372056f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = [e for e in flairToposText['entity_id'].unique() if isinstance(e, int)]\n",
    "entity_id_column = []\n",
    "flair_ner_column = []\n",
    "topostext_id_column = []\n",
    "\n",
    "for n in tqdm(range(min(entity_ids), max(entity_ids)+1)):\n",
    "    entity_id_column.append(n)\n",
    "    filter_entity = flairToposText[flairToposText['entity_id'] == n]\n",
    "    filter_entity = filter_entity.reset_index()\n",
    "    entity_ner = re.findall(r\"B-([A-Z]+)\", filter_entity['flair_ner'][0])\n",
    "    flair_ner_column.append(entity_ner[0])\n",
    "    for i,token in enumerate(filter_entity['token']):\n",
    "        flag = False\n",
    "        if filter_entity['topostext'][i] != '-':\n",
    "            topostext_id_column.append(filter_entity['topostext'][i])\n",
    "            flag = True\n",
    "            break        \n",
    "    if flag == False:\n",
    "        topostext_id_column.append('-')\n",
    "\n",
    "UniqueEntities = pd.DataFrame(\n",
    "{'entity_id': entity_id_column,\n",
    "'topostext_id': topostext_id_column,\n",
    "'flair_ner' : flair_ner_column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9b173-8f05-433d-9462-4b9c48a14d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = ['.', '!', '?']\n",
    "sentence_column = []\n",
    "first_book = min(flairToposText['book'])\n",
    "last_book = max(flairToposText['book'])\n",
    "for i1 in range(first_book, last_book+1):\n",
    "    filter = flairToposText[flairToposText['book'] == i1]\n",
    "    filter = filter.reset_index()\n",
    "    count_sentence = 0    \n",
    "    for i2,token in enumerate(filter['token']):\n",
    "        sentence_column.append(count_sentence)    \n",
    "        if token in punctuations: \n",
    "            count_sentence = count_sentence + 1\n",
    "flairToposText['sentence'] = sentence_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c7d5c-c264-4ef4-968f-4a93a342ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here you can save as .csv the annotated English translation\n",
    "flairToposText.drop(columns=['topostext']).to_csv(main_folder+'/'+work+'_eng.csv', sep= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dbbae-78a5-4446-ae92-0f8b32bf70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(first_book, last_book+1):\n",
    "    filter_book = flairToposText[flairToposText['book'] == i]\n",
    "    name= main_folder+'/books/engbook'+str(i)\n",
    "    with open(name, \"w\", encoding='utf-8') as file: \n",
    "        for i in range(0, filter_book['sentence'].max()+1):\n",
    "            filter_rows = filter_book[filter_book['sentence'] == i]\n",
    "            concatenate_sentence = ' '.join(filter_rows['token'].astype(str))\n",
    "            file.write(f\"{concatenate_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc736e3-aa41-4be0-a927-eef55bbe9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_perseus = requests.get(url_perseus)\n",
    "soup_perseus = BeautifulSoup(access_perseus.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33fe64-5243-4dff-96d5-3500f166c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = soup_perseus.find_all(\"div\", type=\"textpart\")\n",
    "reference_column = []\n",
    "book_column = []\n",
    "token_column = []\n",
    "for chapter in chapters:\n",
    "    reference = chapter.get(\"n\")\n",
    "    p_tag = chapter.find(\"p\") \n",
    "    p_contents = p_tag.get_text()\n",
    "    words = p_contents.split()\n",
    "    for i, word in enumerate(words):\n",
    "        #reference = w_tag.get(\"n\")\n",
    "        reference_column.append(reference)\n",
    "        #match = re.match(r\"^\\d+\", reference)\n",
    "        #if match:\n",
    "        #  book = int(match.group())\n",
    "        book = int(reference)\n",
    "        book_column.append(book)\n",
    "        #token = w_tag.get_text()\n",
    "        token = word\n",
    "        token_column.append(token)\n",
    "\n",
    "#data = {'reference': reference_column, 'book':book_column, 'token': token_column}\n",
    "data = {'reference': reference_column, 'book':book_column, 'token': token_column}\n",
    "GreekText = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2faed66-8281-4f17-bde3-884243605c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_column = []\n",
    "first_book = min(GreekText['book'])\n",
    "last_book = max(GreekText['book'])\n",
    "for i1 in range(first_book, last_book+1):\n",
    "    filter_book = GreekText[GreekText['book'] == i1]\n",
    "    filter_book = filter_book.reset_index()\n",
    "    count_sentence = 0    \n",
    "    for i2,token in enumerate(filter_book['token']):\n",
    "        sentence_column.append(count_sentence)    \n",
    "        if token in punctuations: \n",
    "            count_sentence = count_sentence + 1\n",
    "GreekText['sentence'] = sentence_column\n",
    "GreekText['index'] = GreekText.index\n",
    "GreekText['entity_id'] = '-' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b784e3-3dd6-4caa-b30b-e01a75d67b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(first_book, last_book+1):\n",
    "    filter_book = GreekText[GreekText['book'] == i]\n",
    "    name= main_folder+'/books/grcbook'+str(i)\n",
    "    with open(name, \"w\", encoding='utf-8') as file: \n",
    "        for i in range(0, filter_book['sentence'].max()+1):\n",
    "            filter_rows = filter_book[filter_book['sentence'] == i]\n",
    "            concatenate_sentence = ' '.join(filter_rows['token'].astype(str))\n",
    "            file.write(f\"{concatenate_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb8bc7-5073-4fbd-ac4a-e6fe80d901d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import overlap\n",
    "from utils.dp_core import *\n",
    "from setuptools import distutils\n",
    "from utils import vecalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda581d-99e4-4303-af87-5a418b159e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(first_book,last_book+1)): \n",
    "    overlap.make_overlap(source = main_folder+'/books/engbook'+str(i), \n",
    "                     target = main_folder+'/books/engoverlaps'+str(i)+'.eng', \n",
    "                     num_overlaps = 10\n",
    "                     )\n",
    "    overlap.make_overlap(source = main_folder+'/books/grcbook'+str(i), \n",
    "                     target = main_folder+'/books/grcoverlaps'+str(i)+'.grc', \n",
    "                     num_overlaps = 10\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b9211-d813-4c13-a5e1-396a757c5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825366d-cac5-4e9c-b107-10a375a99190",
   "metadata": {},
   "outputs": [],
   "source": [
    "renew_labse_files = False\n",
    "basedir = main_folder+'/books'\n",
    "lang1 = 'eng'\n",
    "lang2 = 'grc'\n",
    "files = os.listdir(basedir)\n",
    "files = [f for f in files if os.path.isfile(basedir+'/'+f) and f.strip().endswith(lang1)]\n",
    "all_embeddings = []\n",
    "for i in files:    \n",
    "    path = basedir+'/'+i\n",
    "    print(path)\n",
    "    with open(path, \"r\", encoding=\"utf8\") as engfile:\n",
    "        overlap_sentences_eng = []\n",
    "        for line in engfile: \n",
    "            overlap_sentences_eng.append(line)\n",
    "    path = path.replace('engoverlaps', 'grcoverlaps')\n",
    "    path = path.replace('.eng', '.grc')\n",
    "    print(path)\n",
    "    with open(path, \"r\", encoding=\"utf8\") as grcfile:\n",
    "        overlap_sentences_grc = []\n",
    "        for line in grcfile:  \n",
    "            overlap_sentences_grc.append(line)\n",
    "    j = i.split('.')[0]\n",
    "    j = j.replace('engoverlaps', '').strip()\n",
    "    src_embedded_filename = 'overlaps.'+lang1+'.'+j+'.LaBSE.emb'\n",
    "    tgt_embedded_filename = 'overlaps.'+lang2+'.'+j+'.LaBSE.emb'\n",
    "    engoverlap_missing = not os.path.exists(main_folder+'/labse_embeddings/overlaps.eng.'+str(j)+'.LaBSE.emb')\n",
    "    grcoverlap_missing = not os.path.exists(main_folder+'/labse_embeddings/overlaps.grc.'+str(j)+'.LaBSE.emb')\n",
    "    if renew_labse_files or engoverlap_missing: \n",
    "        print(f'making english embeddings: {len(overlap_sentences_eng)}')\n",
    "        overlaps_embeddings_eng_LaBSE = model.encode(overlap_sentences_eng)\n",
    "        overlaps_embeddings_eng_LaBSE.tofile(main_folder+'/labse_embeddings/overlaps.eng.'+str(j)+'.LaBSE.emb')\n",
    "    else:\n",
    "        print('Skipping creation of '+src_embedded_filename+'. Embeddings exist and cell is in no-ovverride mode.')\n",
    "    if renew_labse_files or grcoverlap_missing:\n",
    "        print(f'making greek embeddings: {len(overlap_sentences_grc)}')\n",
    "        overlaps_embeddings_grc_LaBSE = model.encode(overlap_sentences_grc)\n",
    "        overlaps_embeddings_grc_LaBSE.tofile(main_folder+'/labse_embeddings/overlaps.grc.'+str(j)+'.LaBSE.emb')\n",
    "    else:\n",
    "        print('Skipping creation of '+tgt_embedded_filename+'. Embeddings exist and cell is in no-ovverride mode.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ed10d-a564-4f95-979b-3c22d7af1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "embeddingsfolder = os.path.join(dir, main_folder,'labse_embeddings')\n",
    "bookids = [book.split('.')[2] for book in os.listdir(embeddingsfolder) if '.eng.' in book]\n",
    "for book in tqdm(bookids): \n",
    "    i = os.path.join(dir, main_folder, 'books', 'engbook'+book)\n",
    "    path = os.path.join(dir, main_folder, 'books', 'grcbook'+book)\n",
    "    src_embedded_filename = main_folder+'/labse_embeddings/overlaps.eng.'+book+'.LaBSE.emb'\n",
    "    tgt_embedded_filename = main_folder+'/labse_embeddings/overlaps.grc.'+book+'.LaBSE.emb'\n",
    "    vecalign.vecalign_custom(\n",
    "            src=i, \n",
    "            tgt=path, \n",
    "            src_embed=(main_folder+'/books/engoverlaps'+book+'.eng', src_embedded_filename), \n",
    "            tgt_embed=(main_folder+'/books/grcoverlaps'+book+'.grc', tgt_embedded_filename) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b01bfc-f7b3-4dfa-8c23-ec77498d36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, XLMRobertaModel, XLMRobertaTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07e229-943c-4d41-bfe9-dccb9746349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"UGARIT/grc-alignment\"\n",
    "device=torch.device('cpu')\n",
    "layer: int=8\n",
    "distortion: float = 0.0\n",
    "config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "emb_model = AutoModelForMaskedLM.from_pretrained(model, config=config)\n",
    "emb_model.eval()\n",
    "emb_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ee650-6049-410b-a829-610200cd8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51d7b1-ff5f-4cb2-a8df-f449ec2298b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_list(sent_batch) -> torch.Tensor:\n",
    "        if emb_model is not None:\n",
    "            with torch.no_grad():\n",
    "                if not isinstance(sent_batch[0], str):\n",
    "                    inputs = tokenizer(sent_batch, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\") ## tokenize the sentence\n",
    "                else:\n",
    "                    inputs = tokenizer(sent_batch, is_split_into_words=False, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                hidden = emb_model(**inputs.to(device))[\"hidden_states\"]  ## create the embeddings\n",
    "                if layer >= len(hidden):\n",
    "                    raise ValueError(f\"Specified to take embeddings from layer {layer}, but model has only {len(hidden)} layers.\")\n",
    "                outputs = hidden[layer]\n",
    "                return outputs[:, 1:-1, :]\n",
    "        else:\n",
    "            return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7516ba-aa42-46ea-a9a6-4e32b43453fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return (cosine_similarity(X, Y) + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28de2e-7c92-4645-812d-764056b7e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_max(sim_matrix: np.ndarray, max_count: int=2) -> np.ndarray:\n",
    "    alpha_ratio = 0.9\n",
    "    m, n = sim_matrix.shape\n",
    "    forward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
    "    backward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
    "    inter = forward * backward.transpose()\n",
    "    if min(m, n) <= 2:\n",
    "        return inter\n",
    "    new_inter = np.zeros((m, n))\n",
    "    count = 1\n",
    "    while count < max_count:\n",
    "        mask_x = 1.0 - np.tile(inter.sum(1)[:, np.newaxis], (1, n)).clip(0.0, 1.0)\n",
    "        mask_y = 1.0 - np.tile(inter.sum(0)[np.newaxis, :], (m, 1)).clip(0.0, 1.0)\n",
    "        mask = ((alpha_ratio * mask_x) + (alpha_ratio * mask_y)).clip(0.0, 1.0)\n",
    "        mask_zeros = 1.0 - ((1.0 - mask_x) * (1.0 - mask_y))\n",
    "        if mask_x.sum() < 1.0 or mask_y.sum() < 1.0:\n",
    "            mask *= 0.0\n",
    "            mask_zeros *= 0.0\n",
    "        new_sim = sim_matrix * mask\n",
    "        fwd = np.eye(n)[new_sim.argmax(axis=1)] * mask_zeros\n",
    "        bac = np.eye(m)[new_sim.argmax(axis=0)].transpose() * mask_zeros\n",
    "        new_inter = fwd * bac\n",
    "        if np.array_equal(inter + new_inter, inter):\n",
    "                break\n",
    "        inter = inter + new_inter\n",
    "        count += 1\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42456d-16f6-49c3-a1d5-aff145ee6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"data/intermediate/alignments/sentence_pairs_alignment\"\n",
    "pattern = r'\\[(\\d+(?:,\\s*\\d+)*)\\](?:,\\s*\\[(\\d+(?:,\\s*\\d+)*)\\])?.*$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac200c5-37bb-4fb1-b03d-bb40f6e04b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d53bc4-3a9a-40ab-9b4a-47a1bf343fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for book_n,df in flairToposText.groupby('book'): \n",
    "    print(book_n)\n",
    "    file_name = path+str(book_n)+'.txt' \n",
    "    filter_book_eng = df\n",
    "    filter_book_grc = GreekText[GreekText['book'] == book_n]\n",
    "    with open(file_name, \"r\") as file:\n",
    "        for line in file:\n",
    "            match1 = re.search(pattern,line) \n",
    "            index_eng = match1[1] \n",
    "            index_grc = match1[2] \n",
    "            if index_eng and index_grc: \n",
    "                index_eng = index_eng.split(',')\n",
    "                index_eng = [int(i) for i in index_eng]\n",
    "                filter_sentence_eng = filter_book_eng[filter_book_eng['sentence'].isin(index_eng)]\n",
    "                filter_sentence_eng = filter_sentence_eng.reset_index() \n",
    "                src_sent = []\n",
    "                indexes_to_project = []\n",
    "                ent_ids = []\n",
    "                for i, engtoken in enumerate(filter_sentence_eng['token']): \n",
    "                    src_sent.append(engtoken) \n",
    "                    ent_ids.append(filter_sentence_eng['entity_id'][i])\n",
    "                    if len(filter_sentence_eng['flair_ner'][i]) > 1:\n",
    "                        indexes_to_project.append(i)\n",
    "                if len(indexes_to_project) > 0:        \n",
    "                    index_grc = index_grc.split(',')\n",
    "                    index_grc = [int(i) for i in index_grc]\n",
    "                    filter_sentence_grc = filter_book_grc[filter_book_grc['sentence'].isin(index_grc)]\n",
    "                    filter_sentence_grc = filter_sentence_grc.reset_index()\n",
    "                    trg_sent = []\n",
    "                    indexes_to_update = []\n",
    "                    for i, grctoken in enumerate(filter_sentence_grc['token']):\n",
    "                        trg_sent.append(grctoken)  \n",
    "                        indexes_to_update.append(filter_sentence_grc['index'][i])             \n",
    "                    l1_tokens = [tokenizer.tokenize(word) for word in src_sent] \n",
    "                    l2_tokens = [tokenizer.tokenize(word) for word in trg_sent] \n",
    "                    bpe_lists = [[bpe for w in sent for bpe in w] for sent in [l1_tokens, l2_tokens]]\n",
    "                    l1_b2w_map = []\n",
    "                    for i, wlist in enumerate(l1_tokens):\n",
    "                        l1_b2w_map += [i for x in wlist]\n",
    "                    l2_b2w_map = []\n",
    "                    for i, wlist in enumerate(l2_tokens):\n",
    "                        l2_b2w_map += [i for x in wlist]\n",
    "                    vectors = get_embed_list([src_sent, trg_sent]).cpu().detach().numpy()\n",
    "                    vectors = [vectors[i, :len(bpe_lists[i])] for i in [0, 1]]\n",
    "                    all_mats = {} \n",
    "                    sim = get_similarity(vectors[0], vectors[1])\n",
    "                    all_mats[\"itermax\"] = iter_max(sim) \n",
    "                    aligns = {} \n",
    "                    aligns['itermax'] = set()\n",
    "                    for i in range(len(vectors[0])):\n",
    "                        for j in range(len(vectors[1])):\n",
    "                            if all_mats['itermax'][i, j] > 0:\n",
    "                                aligns['itermax'].add((l1_b2w_map[i], l2_b2w_map[j]))\n",
    "                    aligns['itermax'] = sorted(aligns['itermax'])                   \n",
    "                    for index_to_project in indexes_to_project:\n",
    "                        projection = 0\n",
    "                        ent_id = ent_ids[index_to_project]\n",
    "                        for wordpair in aligns['itermax']: \n",
    "                            i_engword = int(wordpair[0]) \n",
    "                            if index_to_project == i_engword:\n",
    "                                i_grcword = int(wordpair[1])\n",
    "                                GreekText['entity_id'][indexes_to_update[i_grcword]] = ent_id \n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd3110-435b-4ca1-bbe4-ae74f011b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "topostext_id_column = []\n",
    "external_id_column = []\n",
    "\n",
    "requestamount = len(list(enumerate(UniqueEntities.topostext_id.unique())))\n",
    "for i, topostext_id in tqdm(enumerate(UniqueEntities.topostext_id.unique()), total = requestamount):\n",
    "    if topostext_id != '-':\n",
    "        topostext_id_column.append(topostext_id)\n",
    "        if topostext_id.startswith('/'):\n",
    "            topostext_id = 'https://topostext.org'+topostext_id\n",
    "        topostext_page = requests.get(topostext_id)\n",
    "        soup = BeautifulSoup(topostext_page.content)\n",
    "        external_link = soup.find('a', href=lambda x: x and 'wikidata.org' in x)\n",
    "        if external_link:\n",
    "            external_href = external_link['href']\n",
    "            external_id_column.append(external_href)\n",
    "        else: external_id_column.append('-')\n",
    "\n",
    "UniqueToposTextIds = pd.DataFrame(\n",
    "{'topostext_id': topostext_id_column,\n",
    "'external_id': external_id_column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a38b27-33eb-499a-860d-54a2b456c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GreekText['flair_ner'] = '-'\n",
    "GreekText['topostext'] = '-'\n",
    "GreekText['wikidata'] = '-'\n",
    "\n",
    "for i,ent_id in enumerate(UniqueEntities['entity_id']):\n",
    "    filter_entity = GreekText[GreekText['entity_id'] == ent_id]\n",
    "    filter_entity.reset_index(inplace=True)\n",
    "    for n,flair_ner in enumerate(filter_entity['flair_ner']):\n",
    "        index_to_update = filter_entity['index'][n]\n",
    "        GreekText['topostext'][index_to_update] = UniqueEntities['topostext_id'][i]\n",
    "        if n == 0:\n",
    "            GreekText['flair_ner'][index_to_update] = 'B-'+UniqueEntities['flair_ner'][i]\n",
    "        else : GreekText['flair_ner'][index_to_update] = 'I-'+UniqueEntities['flair_ner'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fad5b6-6da6-45f6-b4db-1e480ef37bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topostext_id in enumerate(UniqueToposTextIds['topostext_id']):\n",
    "    filter_topostext = GreekText[GreekText['topostext'] == topostext_id]\n",
    "    filter_topostext.reset_index(inplace=True)\n",
    "    for n,flair_ner in enumerate(filter_topostext['flair_ner']):\n",
    "        index_to_update = filter_topostext['index'][n]\n",
    "        GreekText['wikidata'][index_to_update] = UniqueToposTextIds['external_id'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af725a-55f3-4135-bfc3-bf84c1b09fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "GreekText.drop(columns=['index']).to_csv(main_folder+'/'+work+'_grc.csv', sep= ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
